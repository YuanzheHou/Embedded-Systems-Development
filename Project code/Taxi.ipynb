{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Smart Cart Problem solving using Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 install the gym environment \n",
    "- conda install -c conda-forge gym\n",
    "\n",
    "### 1.1 load the game environment and render what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 reset environment to a new, random state and render what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "The Action Space Discrete(6)\n",
      "The State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "print(\"The Action Space {}\".format(env.action_space))\n",
    "print(\"The State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Representations\n",
    "- The filled square represents the taxi. Yellow means there's no passenger, green means there's a passenger on the taxi\n",
    "- The pipe (\"|\") represents a wall that the taxi cannot cross.\n",
    "- R, G, Y, B are the possible pickup and destination locations. The blue letter means the current passenger pick-up location, and the purple letter represents the current destination.\n",
    "- The action number is from 0-5 where 0 = south, 1 = north, 2 = east, 3 = west, 4 = pickup, 5 = dropoff\n",
    "\n",
    "As verified by the prints, we have an Action Space of size 6 and a State Space of size 500 = 5 * 5 parking lots, 4 destinations and (4+1) passenger locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Encode the state and give it to the environment to render in Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 228\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The parameters of encode is (taxi row, taxi column, passenger index, destination index)\n",
    "state = env.encode(2, 1, 2, 0)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 The reward Table\n",
    "\n",
    "When the Taxi environment is created, there is an initial Reward table that's also created, called `P`. We can think of it like a matrix that has the number of states as rows and number of actions as columns, i.e. a states × actions matrix.\n",
    "\n",
    "- The 0-5 corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at our current state in the illustration.\n",
    "- In this env, probability is always 1.0.\n",
    "- The nextstate is the state we would be in if we take the action at this index of the dict.\n",
    "- All the movement actions have a -1 reward and the pickup/dropoff actions have -10 reward in this particular state. If we are in a state where the taxi has a passenger and is on top of the right destination, we would see a reward of 20 at the dropoff action (5).\n",
    "- done is used to tell us when we have successfully dropped off a passenger in the right location. Each successfull dropoff is the end of an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 328, -1, False)],\n",
       " 1: [(1.0, 128, -1, False)],\n",
       " 2: [(1.0, 248, -1, False)],\n",
       " 3: [(1.0, 208, -1, False)],\n",
       " 4: [(1.0, 228, -10, False)],\n",
       " 5: [(1.0, 228, -10, False)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the structure of the dictionary P is {action: [(probability, nextstate, reward, done)]}\n",
    "env.P[228]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Solving the environment without Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1 Process\n",
    "\n",
    "- Create an infinite loop which runs until one passenger reaches one destination (one episode), or when the received reward is 20. \n",
    "\n",
    "- The env.action_space.sample() method automatically selects one random action from set of all possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Timesteps taken: 4999\n",
      "The Penalties incurred: 1632\n"
     ]
    }
   ],
   "source": [
    "# set environment to illustration's state\n",
    "env.s = 328\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "frames = []\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "    epochs += 1\n",
    "    \n",
    "print(\"The Timesteps taken: {}\".format(epochs))\n",
    "print(\"The Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 Show all the states for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "The Timestep: 4999\n",
      "The State: 0\n",
      "The Action: 5\n",
      "The Reward: 20\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"{frame['frame']}\") \n",
    "        print(f\"The Timestep: {i + 1}\")\n",
    "        print(f\"The State: {frame['state']}\")\n",
    "        print(f\"The Action: {frame['action']}\")\n",
    "        print(f\"The Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.3 Result Analysis\n",
    "\n",
    "Because we never learn from past experience, it takes too many steps. Although we run this over and over, it will never be optimized. The agent has no memory of which action was best for each state, which is exactly what Reinforcement Learning will do for us. Next, we want to use Reinforcement Learning to get the optimization solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Reinforcement Learning Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing up the Q-Learning Process:\n",
    "- Initialize the Q-table by all zeros.\n",
    "- Start exploring actions: For each state, select any one among all possible actions for the current state (S).\n",
    "- Travel to the next state (S') as a result of that action (a).\n",
    "- For all possible actions from the state (S') select the one with the highest Q-value.\n",
    "- Update Q-table values using the equation.\n",
    "- Set the next state as the current state.\n",
    "- If goal state is reached, then end and repeat the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.1 Initialize the Q-table to a 500×6 matrix of zeros\n",
    "\n",
    "The Q table (0,1,2,3,4,5)\n",
    "- 0 represents South\n",
    "- 1 represents North\n",
    "- 2 represents East\n",
    "- 3 represents West\n",
    "- 4 represents pickup\n",
    "- 5 represents dropoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "(500, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "print(q_table)\n",
    "print(q_table.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.2 Training the agent\n",
    "- Create the training algorithm and update this Q-table as the agent explores the environment over thousands of episodes.\n",
    "- Decide whether to pick a random action or to exploit the already computed Q-values by using the epsilon value and comparing it to the random.uniform(0, 1) function, which returns an arbitrary number between 0 and 1.\n",
    "- Execute the chosen action in the environment to obtain the next_state and the reward from performing the action.\n",
    "- Calculate the maximum Q-value for the actions corresponding to the next_state, and update our Q-value to the new_q_value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 150000\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 150001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        Pre_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * Pre_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.3639511 ,  -2.3639511 ,  -2.3639511 ,  -2.1220864 ,\n",
       "       -11.27325178, -11.27325176])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[228]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.3 Result Analysis\n",
    "\n",
    "- The Q-table has been established over 150,000 episodes, and the Q-values are shwon as above.\n",
    "- The maximum Q value is \"west\" (-2.1220864), so it looks like Q-Learning has effectively learned the best action to take in the illustration state!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.4 evaluating the agent\n",
    "\n",
    "- Needn't to explore actions any further and the next action is always selected using the best Q-value.\n",
    "\n",
    "- From the evaluation, the agent's performance improved significantly and it incurred no penalties, meaning it performed the correct pickup/dropoff actions with 500 different passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 500 episodes:\n",
      "Average timesteps per episode: 13.004\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 500\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
